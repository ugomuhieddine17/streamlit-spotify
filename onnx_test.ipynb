{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import sys \n",
    "import base64\n",
    "from music_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as ntx\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "from music_utils import *\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch_geometric.nn as graphnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(DATA_PATH + 'best-model_very_good_92accu.pt',  map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/Users/muhieddineugo/opt/miniconda3/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-darwin.so'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4abac013331e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspot_600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martist_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_featurings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date_spotify_600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_spot_artists_600\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_initialisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4abac013331e>\u001b[0m in \u001b[0;36mfull_initialisation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mspotify_600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists_600\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_spotify_600\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents_local/CS-DSBA/CS/MLNS/streamlit-spotify/music_utils.py\u001b[0m in \u001b[0;36mread_spotify_600\u001b[0;34m(DATA_PATH, read, spotify_path, artists_path, pkl_spotify_path, pkl_artists_path)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mspotify_600\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpkl_spotify_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0martists_600\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpkl_artists_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;31m#  \"No module named 'pandas.core.sparse.series'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;31m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/compat/pickle_compat.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_verbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1208\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36mload_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STACK_GLOBAL requires str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTACK_GLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stack_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/pandas/compat/pickle_compat.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_class_locations_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             raise AttributeError(\"Can't get attribute {!r} on {!r}\"\n\u001b[0m\u001b[1;32m    332\u001b[0m                                  .format(name, obj)) from None\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/Users/muhieddineugo/opt/miniconda3/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-darwin.so'>"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "################## PACKAGES #########################\n",
    "#####################################################\n",
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import sys \n",
    "import base64\n",
    "from music_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as ntx\n",
    "import csv\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "from music_utils import *\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch_geometric.nn as graphnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import altair as alt\n",
    "from altair import expr, datum\n",
    "\n",
    "\n",
    "local = True\n",
    "colab = False\n",
    "git = False\n",
    "if local:\n",
    "    DATA_PATH = './data/'\n",
    "\n",
    "elif git:\n",
    "    DATA_PATH = './data/'\n",
    "    \n",
    "elif colab:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    DATA_PATH = './gdrive/MyDrive/MLNS_Spotify/data/'\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    sys.path.append('/content/gdrive/MyDrive/MLNS_Spotify')\n",
    "\n",
    "PATH_TRAIN = DATA_PATH+\"train.txt\"\n",
    "PATH_NODE_INFO = DATA_PATH+\"node_information.csv\"\n",
    "PATH_TEST = DATA_PATH+\"test.txt\"\n",
    "# sys.path.append('/content/gdrive/MyDrive/MLNS_Spotify')\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP_post(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        # torch.manual_seed(12345)\n",
    "        self.num_layers = num_layers\n",
    "        self.lin1 = Linear(in_channels, hidden_channels)\n",
    "        self.list_FC = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.list_FC.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.last_lin = Linear(hidden_channels, out_channels)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = F.elu(self.list_FC[i](x))\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return self.softmax(self.last_lin(x))\n",
    "        \n",
    "\n",
    "class GAT_MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, embed_size, n_heads,  MLP_num_layers, MLP_hidden_channels, MLP_out_channels, dropout=False):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.graphconv1 = graphnn.conv.GATConv(in_channels=in_channels, out_channels=hidden_channels, heads=n_heads, concat=True)\n",
    "        self.list_GATC = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.list_GATC.append(graphnn.conv.GATConv(in_channels=n_heads*hidden_channels, out_channels=hidden_channels, heads=n_heads, concat=True))\n",
    "        \n",
    "        self.last_conv = graphnn.conv.GATConv(in_channels=n_heads*hidden_channels, out_channels=embed_size, heads=n_heads, concat=False)\n",
    "\n",
    "        self.elu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.MLP_post = MLP_post(in_channels=embed_size*2, hidden_channels=MLP_hidden_channels, num_layers=MLP_num_layers, out_channels=MLP_out_channels)\n",
    "                  \n",
    "    def forward(self, x, edge_index):\n",
    "          # print('very first', x)\n",
    "        # TO AVOID NAN PROPAGATION\n",
    "          x = torch.nan_to_num(x)\n",
    "          # print('first', x)\n",
    "          x = self.graphconv1(x, edge_index)\n",
    "        #   print('x beginnning', x)\n",
    "          x = self.elu(x)\n",
    "          for i in range(self.num_layers):\n",
    "              x = x + self.elu(self.list_GATC[i](x, edge_index))\n",
    "              x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "          x = self.last_conv(x, edge_index).relu()\n",
    "          # print('end GAT', x)\n",
    "          #we concatenate\n",
    "          # Extract the source and target node indices from edge_index\n",
    "          src_idx = edge_index[0]\n",
    "          tgt_idx = edge_index[1]\n",
    "\n",
    "          # Use indexing to extract the node features for the source and target nodes\n",
    "          src_features = x[src_idx]\n",
    "          tgt_features = x[tgt_idx]\n",
    "\n",
    "          # Concatenate the features along the last dimension\n",
    "          x = torch.cat([src_features, tgt_features], dim=-1)\n",
    "          # print(edge_attr.size())\n",
    "          # print('concatenated', x.size())\n",
    "          x = self.MLP_post.forward(x)\n",
    "          # print('final', x)\n",
    "        #   print('x',x)\n",
    "          return x\n",
    "\n",
    "\n",
    "def full_initialisation():\n",
    "    ######################################################\n",
    "    ######################################################\n",
    "    ##               DATA INITIALISATION               ###\n",
    "    ######################################################\n",
    "    ######################################################\n",
    "\n",
    "\n",
    "    spotify_600, artists_600 = read_spotify_600(DATA_PATH=DATA_PATH, read=True)\n",
    "\n",
    "\n",
    "    #####\n",
    "    ## PARAMETERS\n",
    "    #####\n",
    "    start_date = datetime.strptime(\"1999-01-01 00:00:01\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date = datetime.strptime(\"2020-12-31 23:59:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    n_month = 12\n",
    "\n",
    "    # one to one spotify database rearrangement\n",
    "    start_date_spotify_600 = spotify_600[spotify_600.release_date.dt.year >= start_date.year].copy()\n",
    "    spot_600 = start_date_spotify_600.copy()\n",
    "\n",
    "    song_artist_pairs = [(track_id, artist_pair[0], artist_pair[1]) for track_id, artists in spot_600[['track_id', 'id_artists']].values for artist_pair in combinations(set(artists), 2)]\n",
    "    correspondace_spot_600 = pd.DataFrame(song_artist_pairs, columns=['track_id', 'artist_1', 'artist_2'])\n",
    "    spot_600 = pd.merge(correspondace_spot_600, spot_600.drop(columns=['name', 'artists', 'id_artists', 'artist_id']), on='track_id', how='left').copy()\n",
    "\n",
    "    # existing artists \n",
    "    in_spot_artists_600 = artists_600[artists_600.artist_id.isin(start_date_spotify_600.id_artists.explode().unique())].copy()\n",
    "    in_spot_artists_600 = in_spot_artists_600.sort_values(by='name')\n",
    "    \n",
    "\n",
    "    ######################################################\n",
    "    ## BUILD FEATURES OF ARTIST CONSIDERING START DATE ###\n",
    "\n",
    "\n",
    "    #features of the artists\n",
    "    artist_features = artists_features_creation(in_spot_artists_600,\n",
    "                                            start_date_spotify_600,\n",
    "                                            start_date_spotify_600,\n",
    "                                            DATA_PATH, read=False,\n",
    "                                            pkl_features_artist_path='features_artists_PYGT_yt_1999.pkl',\n",
    "                                ).reset_index()\n",
    "\n",
    "    # print(f'len of artist_features : {len(artist_features)}')\n",
    "\n",
    "    ######################################################\n",
    "    ##          ARTIST ID TO INT DICTIONNARY           ###\n",
    "\n",
    "    # Reassign the location IDs (makes it easier later, because here the IDs didn't start at 0)\n",
    "    artist_idname = artist_features['artist_id'].unique()\n",
    "    new_ids = list(range(len(artist_idname)))\n",
    "    mapping = dict(zip(artist_idname, new_ids))\n",
    "    reversed_mapping = dict(zip(new_ids, artist_idname))\n",
    "\n",
    "    artist_features['int_artist_id'] = artist_features['artist_id'].map(mapping)\n",
    "\n",
    "\n",
    "    spot_600['artist_1'] = spot_600['artist_1'].map(mapping)\n",
    "    spot_600['artist_2'] = spot_600['artist_2'].map(mapping)\n",
    "\n",
    "    #We drop potential nans\n",
    "    missings = spot_600[(spot_600.artist_1.isna()) | (spot_600.artist_2.isna())].copy()\n",
    "    spot_600 = spot_600.dropna(subset=['artist_1', 'artist_2']).copy()\n",
    "\n",
    "\n",
    "    int_to_name = dict(artist_features[['int_artist_id', 'name']].values)\n",
    "\n",
    "    spot_600['artist_1_name'] = spot_600.artist_1.map(int_to_name)\n",
    "    spot_600['artist_2_name'] = spot_600.artist_2.map(int_to_name)\n",
    "\n",
    "\n",
    "    df_featurings = spot_600.groupby(['artist_1', 'artist_2']).agg(num_feats=('track_id', 'count')).reset_index()\n",
    "\n",
    "    df_featurings['artist_1_name'] = df_featurings.artist_1.map(int_to_name)\n",
    "    df_featurings['artist_2_name'] = df_featurings.artist_2.map(int_to_name)\n",
    "\n",
    "    node_features = np.array(artist_features.drop(columns=['artist_id', 'genres', 'name', 'int_artist_id']).fillna(0))\n",
    "    node_features = (node_features - node_features.mean(axis=0))/node_features.std(axis=0) \n",
    "\n",
    "    model = torch.load(DATA_PATH + 'best-model_very_good_92accu.pt',  map_location='cpu')\n",
    "\n",
    "    return mapping, reversed_mapping, int_to_name, spot_600, artist_features, df_featurings, node_features, model, start_date_spotify_600, in_spot_artists_600\n",
    "\n",
    "\n",
    "def artist_features_evolving(in_spot_artists_600, start_date_spotify_600, end_date, mapping):\n",
    "\n",
    "    start_end_spot_600 = start_date_spotify_600[start_date_spotify_600.release_date <= end_date].copy()\n",
    "    artist_features = artists_features_creation(in_spot_artists_600,\n",
    "                                            start_date_spotify_600,\n",
    "                                          start_end_spot_600,\n",
    "                                          DATA_PATH, read=False\n",
    "                              ).reset_index()\n",
    "    artist_features['int_artist_id'] = artist_features['artist_id'].map(mapping)\n",
    "    artist_features = artist_features.sort_values(by='int_artist_id')\n",
    "\n",
    "    node_features = np.array(artist_features.drop(columns=['artist_id', 'genres', 'name', 'int_artist_id']).fillna(0))\n",
    "    node_features = (node_features - node_features.mean(axis=0))/node_features.std(axis=0) \n",
    "\n",
    "    return node_features\n",
    "\n",
    "\n",
    "    \n",
    "def test_Data_construction(df_select, node_features):\n",
    "    \"\"\"build the data test obect\n",
    "\n",
    "    Args:\n",
    "        df_select (pandas dataframe): the selected spotify_600 subset\n",
    "        node_features (array): features of the nodes\n",
    "\n",
    "    Returns:\n",
    "        pytorch geometric Data: the test Data object\n",
    "    \"\"\"\n",
    "    \n",
    "    edge_list = torch.from_numpy(np.array(df_select[['artist_1','artist_2']].values).transpose())\n",
    "    edge_attr = torch.from_numpy(np.array(df_select.num_feats.values).transpose())\n",
    "    y = torch.from_numpy(np.array(df_select.done_feat.values).transpose())\n",
    "    test_data = Data(x=torch.from_numpy(node_features).float(), \n",
    "    y_indices=edge_list.long(), \n",
    "    edge_index=edge_list, \n",
    "    edge_attr=edge_attr,\n",
    "    y=y)\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "\n",
    "def visualize_val_prediction(val_data, int_to_name, graph_name='val_graph'):\n",
    "\n",
    "    got_net = Network(height='1000px', width='100%',bgcolor='#222222', font_color='white',\n",
    "    notebook = False, directed=False)\n",
    "\n",
    "    print('bite')\n",
    "    sources = val_data.y_indices[0,:].tolist()\n",
    "    targets = val_data.y_indices[1,:].tolist()\n",
    "    prediction = val_data.prediction.tolist()\n",
    "    true_label = val_data.y.tolist()\n",
    "    \n",
    "    sources = [int_to_name[x] for x in sources]\n",
    "    targets = [int_to_name[x] for x in targets]\n",
    "\n",
    "    # true_label = \n",
    "    edge_data = zip(sources, targets, prediction, true_label) \n",
    "\n",
    "    for e in edge_data:\n",
    "        src = str(e[0])\n",
    "        dst = str(e[1])\n",
    "        pred = e[2]\n",
    "        label = e[3]\n",
    "        got_net.add_node(src, src, title=src)\n",
    "        got_net.add_node(dst, dst, title=dst)\n",
    "    \n",
    "        if pred+label == 2:\n",
    "            got_net.add_edge(src, dst, title=\"TP\", color=\"#38761d\", width=4)\n",
    "\n",
    "        if pred+label == 0:\n",
    "            got_net.add_edge(src, dst, title=\"TN\", color=\"#b6d7a8\", width=4)\n",
    "            pass\n",
    "        elif pred == 0 and label == 1:\n",
    "            got_net.add_edge(src, dst, title=\"FN\", color=\"#f44336\", width=4)\n",
    "\n",
    "        elif pred == 1 and label == 0:\n",
    "            got_net.add_edge(src, dst, title=\"FP\", color=\"#f4cccc\", width=4)\n",
    "\n",
    "    try:\n",
    "        path = '/tmp'\n",
    "        got_net.save_graph(f'{path}/{graph_name}.html')\n",
    "        HtmlFile = open(f'{path}/{graph_name}.html', 'r', encoding='utf-8')\n",
    "        print('tmp')\n",
    "\n",
    "    # Save and read graph as HTML file (locally)\n",
    "    except:\n",
    "        path = './html_files'\n",
    "        got_net.save_graph(f'{path}/{graph_name}.html')\n",
    "        HtmlFile = open(f'{path}/{graph_name}.html', 'r', encoding='utf-8')\n",
    "\n",
    "    # print(artist_net)\n",
    "    # Load HTML file in HTML component for display on Streamlit page\n",
    "    # print(HtmlFile)\n",
    "    raw_html = HtmlFile.read().encode(\"utf-8\")\n",
    "    raw_html = base64.b64encode(raw_html).decode()\n",
    "    components.iframe(f\"data:text/html;base64,{raw_html}\", height=510)#, width=700)\n",
    "\n",
    "\n",
    "def y_labels_val(spot_600, df_select):\n",
    "    labels_df = spot_600[(spot_600.release_date >= begin_date) & (spot_600.release_date <= end_date)].copy()\n",
    "    \n",
    "    labels_df = labels_df.groupby(['artist_1_name', 'artist_2_name']).agg(num_feats=('track_id', 'count')).reset_index()\n",
    "    labels_df['done_feat'] = labels_df.num_feats.apply(lambda x: 1 if x >= 1 else 0)\n",
    "    df_select = pd.merge(df_select, labels_df[['artist_1_name', 'artist_2_name', 'done_feat']],\n",
    "                        on=['artist_1_name', 'artist_2_name'],\n",
    "                        how='left'\n",
    "                    )\n",
    "    df_select.done_feat = df_select.done_feat.fillna(0)\n",
    "\n",
    "    return df_select\n",
    "\n",
    "def plot_general_info(start_date_spotify_600, selected_artists, indic):\n",
    "    start_date_spotify_600['year'] = start_date_spotify_600.release_date.dt.year\n",
    "    id_to_name = {a:b for (a,b) in artist_features[['artist_id', 'name']].values}\n",
    "    start_date_spotify_600['artist_name'] = start_date_spotify_600.artist_id.map(id_to_name)\n",
    "    to_means = ['track_popularity', 'duration_ms', 'explicit',\n",
    "            'danceability', 'energy',\n",
    "        'key', 'loudness', 'speechiness', 'acousticness',\n",
    "        'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'Number of artists involved']\n",
    "    dico_agg = {a:'mean' for a in to_means}\n",
    "    dico_agg['number of tracks'] = 'count'\n",
    "\n",
    "    start_date_spotify_600 = start_date_spotify_600.rename(columns={'track_id':'number of tracks', 'num_artists':'Number of artists involved'})\n",
    "    \n",
    "    useful_one = start_date_spotify_600[start_date_spotify_600.artist_name == selected_artists[0]].copy()\n",
    "    agg_useful = useful_one.groupby('year').agg(dico_agg).reset_index()\n",
    "\n",
    "    chart = alt.Chart(agg_useful).mark_point().encode(\n",
    "        x='year:O',\n",
    "        y=alt.Y(f'{indic}:Q', scale=alt.Scale(domain=[agg_useful[indic].min()*0.8, agg_useful[indic].max()*1.1])),\n",
    "        tooltip=[\n",
    "            alt.Tooltip('Year:O', title='Year'),\n",
    "            alt.Tooltip(f'number of tracks:Q'),\n",
    "        ]\n",
    "        ).properties(\n",
    "                width=500,\n",
    "                height=300\n",
    "                )\n",
    "    chart += alt.Chart(agg_useful).mark_line().encode(\n",
    "        x='year:O',\n",
    "        y=alt.Y(f'{indic}:Q', scale=alt.Scale(domain=[agg_useful[indic].min()*0.8, agg_useful[indic].max()*1.1])),\n",
    "        tooltip=[\n",
    "            alt.Tooltip('Year:O', title='Year'),\n",
    "            alt.Tooltip(f'number of tracks:Q'),\n",
    "        ]\n",
    "        ).properties(\n",
    "                width=400,\n",
    "                height=600\n",
    "                ).interactive()\n",
    "    return chart\n",
    "\n",
    "mapping, reversed_mapping, int_to_name, spot_600, artist_features, df_featurings, node_features, model, start_date_spotify_600, in_spot_artists_600 = full_initialisation()\n",
    "\n",
    "\n",
    "######################################################\n",
    "######################################################\n",
    "##          GRAPH USER PARAMETERIZATION            ###\n",
    "######################################################\n",
    "######################################################\n",
    "\n",
    "# Set header title\n",
    "st.title('The future of Music')\n",
    "# Define list of selection options and sort alphabetically\n",
    "artist_list = artist_features.name.unique()\n",
    "\n",
    "#Define the list of genres\n",
    "genres_list = artist_features.genres.explode().unique()\n",
    "\n",
    "# Define list of selection options and sort alphabetically\n",
    "artist_list = ['Damso']\n",
    "\n",
    "#Define the list of genres\n",
    "genres_list = []\n",
    "\n",
    "\n",
    "graph_type = 'Direct connections'\n",
    "begin_date = date(1999, 1, 1)\n",
    "\n",
    "end_date = date(2022, 1, 1)\n",
    "\n",
    "begin_date = np.datetime64(begin_date)\n",
    "end_date = np.datetime64(end_date)\n",
    "\n",
    "selected_artists = ['Damso']\n",
    "\n",
    "df_select = df_featurings.loc[df_featurings['artist_1_name'].isin(selected_artists) | \\\n",
    "                            df_featurings['artist_2_name'].isin(selected_artists)]\n",
    "df_select = df_select.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "## GENERAL INFO ####\n",
    "####################\n",
    "\n",
    "\n",
    "if len(selected_artists) != 0:\n",
    "    \n",
    "    #dico vals\n",
    "    to_means = ['track_popularity', 'duration_ms', 'explicit',\n",
    "            'danceability', 'energy',\n",
    "        'key', 'loudness', 'speechiness', 'acousticness',\n",
    "        'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'Number of artists involved']\n",
    "    dico_agg = {a:'mean' for a in to_means}\n",
    "    dico_agg['number of tracks'] = 'count'\n",
    "\n",
    "\n",
    "    #streamlit drawings\n",
    "    st.markdown(f'# {selected_artists[0]}, general information \\n ')\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        \n",
    "        the_row = artist_features[artist_features.name == selected_artists[0]]\n",
    "        st.markdown(f'### Popularity \\n \\\n",
    "        predicted: In process \\n  \\\n",
    "        True: {the_row.artist_popularity.values[0]}')\n",
    "        \n",
    "        st.markdown(f'### Genres and followers \\n \\\n",
    "        Followers: {the_row.followers.values[0]}  \\n  \\\n",
    "        Genres: {the_row.genres.values[0]}')\n",
    "\n",
    "        \n",
    "\n",
    "    with col2:\n",
    "        st.markdown('### Overtime, what changed?')\n",
    "        indic = st.selectbox(\n",
    "            \"What  information would you like to see?\",\n",
    "            list(dico_agg.keys()),\n",
    "            index=0\n",
    "        )\n",
    "\n",
    "        chart = plot_general_info(start_date_spotify_600, selected_artists, indic)\n",
    "        st.altair_chart(chart, use_container_width=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # choice = st.number_input(\"Pick the number of most probable featurings\", 0, 50)\n",
    "    st.markdown('## What will be his/her next collaborations?')\n",
    "\n",
    "test_data = test_Data_construction(df_select, node_features)\n",
    "test_pred = model(test_data.x, test_data.y_indices)\n",
    "proba_featuring = test_pred[:,1].tolist()\n",
    "proba_featuring = [round(prob,3) for prob in proba_featuring]\n",
    "sources = test_data.y_indices[0,:].tolist()\n",
    "targets = test_data.y_indices[1,:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cpu/torch_stable.html, https://data.pyg.org/whl/torch-1.12.1+cpu.html, https://pytorch-geometric.com/whl/torch-1.12.1+cpu.html\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.12.1+cpu (from -r requirements.txt (line 4)) (from versions: 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch==1.12.1+cpu (from -r requirements.txt (line 4))\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc895b69e755cdb2d9601c81f5369a2139eccb1bb64c066cd88f550054f4d0ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
